{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/everestso/Fall24Spring25/blob/main/FrozenLake_DirectEvaluation.ipynb","timestamp":1726769811533}],"collapsed_sections":["rTwoHidK7QXR","f_yt-sIhrGwJ","8IhPpFgYebAl","B2k10QKuIEUt","ftq7t-uXf7IC","UAbcrgTKh-KD","bjJtZ_Tcqrcr"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Frozen Lake w/ Value Iteration & Direct Evaluation"],"metadata":{"id":"rTwoHidK7QXR"}},{"cell_type":"markdown","source":["## Frozen Lake Domain Description\n","\n","Frozen Lake is a simple grid-world environment where an agent navigates a frozen lake to reach a goal while avoiding falling into holes. The environment is represented as a grid, with each cell being one of the following:\n","\n","* **S**: Starting position of the agent\n","* **F**: Frozen surface, safe to walk on\n","* **H**: Hole, falling into one ends the episode with a reward of 0\n","* **G**: Goal, reaching it ends the episode with a reward of 1\n","\n","The agent can take four actions:\n","\n","* **0: Left**\n","* **1: Down**\n","* **2: Right**\n","* **3: Up**\n","\n","However, due to the slippery nature of the ice, the agent might not always move in the intended direction. There's a chance it moves perpendicular to the intended direction.\n","\n","\n"],"metadata":{"id":"hzTUHNC0Oien"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nKf_jjk9OgN1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"df843b91-257a-41a2-8589-a6df5b693170"},"outputs":[{"output_type":"stream","name":"stdout","text":["  (Left)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n"]}],"source":["import gym\n","\n","# Create the environment\n","env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n","\n","# Reset the environment to the initial state\n","observation = env.reset()\n","\n","# Take a few actions and observe the results\n","for _ in range(5):\n","    action = env.action_space.sample()  # Choose a random action\n","    observation, reward, done, info = env.step(action)\n","    # Render the environment to see the agent's movement (text-based)\n","    if done:\n","        observation= env.reset()\n","    else:\n","      rendered = env.render()\n","      if len(rendered) > 1:  # Check if there's a second element\n","         print(rendered[1])  # Print the second element\n","# Close the environment\n","env.close()"]},{"cell_type":"markdown","source":["The transition model for the Frozen Lake world describes how the agent's actions affect its movement and the resulting state transitions. Here's a breakdown of the key components:\n","\n","**Actions:**\n","\n","* The agent can choose from four actions:\n","    * 0: Left\n","    * 1: Down\n","    * 2: Right\n","    * 3: Up\n","\n","**State Transitions:**\n","\n","* **Intended Movement:** Ideally, the agent moves one cell in the chosen direction.\n","* **Slippery Ice:** Due to the slippery nature of the ice, there's a probability that the agent will move in a perpendicular direction instead of the intended one. The exact probabilities depend on the specific Frozen Lake environment configuration, but typically:\n","    * **Successful Move:** The agent moves in the intended direction with a high probability.\n","    * **Perpendicular Move:** The agent moves 90 degrees to the left or right of the intended direction with a lower probability.\n","* **Boundaries:** If the intended movement would take the agent outside the grid boundaries, it remains in its current position.\n","* **Holes:** If the agent lands on a hole (\"H\"), the episode ends, and it receives a reward of 0.\n","* **Goal:** If the agent reaches the goal (\"G\"), the episode ends, and it receives a reward of 1.\n","\n","\n"],"metadata":{"id":"R_q5-OvYOldL"}},{"cell_type":"code","source":["import gym\n","\n","# Create the environment\n","env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n","\n","# Reset the environment to the initial state\n","observation = env.reset()\n","\n","# Take a few actions and observe the results\n","for _ in range(5):\n","    action = env.action_space.sample()  # Choose a random action\n","    observation, reward, done, info = env.step(action)\n","    # Render the environment to see the agent's movement (text-based)\n","    if done:\n","        observation= env.reset()\n","    else:\n","      rendered = env.render()\n","      if len(rendered) > 1:  # Check if there's a second element\n","         print(rendered[1])  # Print the second element\n","# Close the environment\n","env.close()\n","print (\"State 14 Going Right: (s, a, r, Done)\", env.P[14][2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E7nU_-9uaOQR","outputId":"cd7dd0a4-7b60-406d-a8c1-1f32b7f3088c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  (Down)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","\n","  (Left)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","\n","State 14 Going Right: (s, a, r, Done) [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n"]}]},{"cell_type":"markdown","source":["# Direct Evaluation\n","\n","## Evaluate Single Episode"],"metadata":{"id":"f_yt-sIhrGwJ"}},{"cell_type":"code","source":["def EvaluateEpisode(env, e, V_DE, V_Counts, gamma=0.9):\n","    future_reward = 0\n","    for t in reversed(e):  # Iterate in reverse order\n","        future_reward = t[3] + gamma * future_reward\n","        V_DE[t[0]] = future_reward+V_DE[t[0]]\n","        V_Counts[t[0]] = V_Counts[t[0]]+1\n","    return V_DE, V_Counts"],"metadata":{"id":"jB40XhParIyD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluate Episode 1\n","\n"],"metadata":{"id":"NRQJmwWJ3E0u"}},{"cell_type":"code","source":["V_DE = np.zeros((env.observation_space.n))\n","V_Counts = np.zeros((env.observation_space.n))\n","V_DE, V_Count = EvaluateEpisode(env, training_episodes[0], V_DE, V_Counts, 0.9)\n","V = np.where(V_Counts != 0, V_DE / V_Counts, 0)\n","print (f\"V_DE=\\n{V_DE.reshape((4,4))}\")\n","print (f\"V_Counts=\\n{V_Counts.reshape((4,4))}\")\n","print (f\"V=\\n{V.reshape((4,4))}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FaUV7MWJ29oe","outputId":"66c223d1-213b-4841-9046-0f46918cf612"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["V_DE=\n","[[0.59049 0.      0.      0.     ]\n"," [0.6561  0.      0.      0.     ]\n"," [0.729   0.81    0.9     0.     ]\n"," [0.      0.      1.      0.     ]]\n","V_Counts=\n","[[1. 0. 0. 0.]\n"," [1. 0. 0. 0.]\n"," [1. 1. 1. 0.]\n"," [0. 0. 1. 0.]]\n","V=\n","[[0.59049 0.      0.      0.     ]\n"," [0.6561  0.      0.      0.     ]\n"," [0.729   0.81    0.9     0.     ]\n"," [0.      0.      1.      0.     ]]\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-40-000a7187090b>:4: RuntimeWarning: invalid value encountered in divide\n","  V = np.where(V_Counts != 0, V_DE / V_Counts, 0)\n"]}]},{"cell_type":"markdown","source":["## Evaluate All Episodes"],"metadata":{"id":"wlbXCln-4EXX"}},{"cell_type":"code","source":["V_DE = np.zeros((env.observation_space.n))\n","V_Counts = np.zeros((env.observation_space.n))\n","for e in training_episodes2:\n","    V_DE, V_Count = EvaluateEpisode(env, e, V_DE, V_Counts, 0.9)\n","V = np.where(V_Counts != 0, V_DE / V_Counts, 0)\n","print (f\"V_DE=\\n{V_DE.reshape((4,4))}\")\n","print (f\"V_Counts=\\n{V_Counts.reshape((4,4))}\")\n","print (f\"V_DirectEvaluation=\\n{np.round(V.reshape((4,4)),2)}\")\n","print (f\"optimal policy= \\n{optimal_policy.reshape((4,4))}\\n optimal_V=\\n{np.round(optimal_V.reshape((4,4)), 2)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9OswEPertn95","outputId":"241425dd-be48-4326-d40f-f6f29dbce79e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["V_DE=\n","[[ 879.44773993   12.02033939   32.45370905    0.        ]\n"," [ 910.67996731    0.           63.889207      0.        ]\n"," [ 943.61931212  887.58499323  418.52594351    0.        ]\n"," [   0.         1065.13267976 1369.46029196    0.        ]]\n","V_Counts=\n","[[12819.   227.   503.     0.]\n"," [ 9779.     0.   646.     0.]\n"," [ 6632.  3600.  1432.     0.]\n"," [    0.  2876.  2159.     0.]]\n","V_DirectEvaluation=\n","[[0.07 0.05 0.06 0.  ]\n"," [0.09 0.   0.1  0.  ]\n"," [0.14 0.25 0.29 0.  ]\n"," [0.   0.37 0.63 0.  ]]\n","optimal policy= \n","[[0 3 0 3]\n"," [0 0 0 0]\n"," [3 1 0 0]\n"," [0 2 1 0]]\n"," optimal_V=\n","[[0.07 0.06 0.07 0.06]\n"," [0.09 0.   0.11 0.  ]\n"," [0.15 0.25 0.3  0.  ]\n"," [0.   0.38 0.64 0.  ]]\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-41-440a84a75783>:5: RuntimeWarning: invalid value encountered in divide\n","  V = np.where(V_Counts != 0, V_DE / V_Counts, 0)\n"]}]},{"cell_type":"markdown","source":["# My Code (Run This Section)"],"metadata":{"id":"NAnLg2NiJZ7A"}},{"cell_type":"markdown","source":["## Value Iteration Code From Previous Assignment"],"metadata":{"id":"8IhPpFgYebAl"}},{"cell_type":"code","source":["import gym\n","import numpy as np\n","\n","# Create FrozenLake environment\n","env = gym.make(\"FrozenLake-v1\")\n","\n","# Value Iteration Algorithm\n","def value_iteration(env, gamma=0.9, num_iterations=1000):\n","    # Initialize value function and policy\n","    V = np.zeros(env.observation_space.n)\n","    policy_value_iteration = np.zeros(env.observation_space.n)\n","\n","    for i in range(num_iterations):\n","        # Create a copy of the current value function\n","        prev_V = np.copy(V)\n","\n","        # Iterate through all states\n","        for state in range(env.observation_space.n):\n","            # Initialize an array to store Q-values for all actions in this state\n","            Q_values = np.zeros(env.action_space.n)\n","\n","            # Iterate through all possible actions\n","            for action in range(env.action_space.n):\n","                # Calculate the expected value of taking this action\n","                for prob, next_state, reward, done in env.P[state][action]:\n","                    Q_values[action] += prob * (reward + gamma * prev_V[next_state])\n","\n","            # Update the value function with the max Q-value\n","            V[state] = max(Q_values)\n","\n","            # Update the policy to choose the action that gives the highest Q-value\n","            policy_value_iteration[state] = np.argmax(Q_values)\n","\n","        # Early stopping condition (optional)\n","        if np.max(np.abs(prev_V - V)) < 1e-6:\n","            break\n","\n","    return V, policy_value_iteration\n","\n","# Apply Value Iteration\n","optimal_V, optimal_policy_value_iteration = value_iteration(env)\n"],"metadata":{"id":"sp8utuwaJhxt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726987481698,"user_tz":420,"elapsed":1246,"user":{"displayName":"Izaiah Benavides","userId":"18087988838344621844"}},"outputId":"9e2daa5d-778a-4b44-95e4-c4e5dedaed72"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"markdown","source":["## Submitted Code for Q-Learning"],"metadata":{"id":"B2k10QKuIEUt"}},{"cell_type":"code","source":["import numpy as np\n","import gym\n","\n","# Create Frozen Lake environment\n","env = gym.make(\"FrozenLake-v1\")\n","\n","# Q-Learning algorithm function\n","def q_learning(env, num_episodes=10000, max_steps=100, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n","    # Initialize Q-table with zeros\n","    Q = np.zeros((env.observation_space.n, env.action_space.n))\n","\n","    # Function for Epsilon-Greedy policy\n","    def epsilon_greedy_action(state, Q, epsilon):\n","        if np.random.rand() < epsilon:\n","            return env.action_space.sample()  # Exploration\n","        else:\n","            return np.argmax(Q[state])  # Exploitation\n","\n","    # Q-Learning loop\n","    for episode in range(num_episodes):\n","        state = env.reset()\n","        done = False\n","\n","        for step in range(max_steps):\n","            # Select action using epsilon-greedy policy\n","            action = epsilon_greedy_action(state, Q, epsilon)\n","\n","            # Perform action and observe the next state and reward\n","            next_state, reward, done, _ = env.step(action)\n","\n","            # Update Q-value\n","            best_next_action = np.argmax(Q[next_state])\n","            Q[state, action] = Q[state, action] + alpha * (reward + gamma * Q[next_state, best_next_action] - Q[state, action])\n","\n","            # Move to the next state\n","            state = next_state\n","\n","            if done:\n","                break\n","\n","        # Decay epsilon to reduce exploration over time\n","        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n","\n","    # Extract the optimal policy from Q-table\n","    optimal_policy = np.argmax(Q, axis=1)\n","    return Q, optimal_policy\n","\n","#Apply Q_learning\n","Q_table, optimal_policy_q_learning = q_learning(env)"],"metadata":{"id":"g4pwHrNfIL6-","collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726987501927,"user_tz":420,"elapsed":20235,"user":{"displayName":"Izaiah Benavides","userId":"18087988838344621844"}},"outputId":"872f96d9-1857-4001-c5d4-18dfa03d34a5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n"]}]},{"cell_type":"markdown","source":["## Evaluate Policy"],"metadata":{"id":"ftq7t-uXf7IC"}},{"cell_type":"code","source":["# Evaluate Policy Function\n","def evaluate_policy(env, policy, num_episodes=1000):\n","    total_reward = 0\n","    for _ in range(num_episodes):\n","        state = env.reset()\n","        done = False\n","        while not done:\n","            action = policy[state]\n","            state, reward, done, _ = env.step(action)\n","            total_reward += reward\n","    return total_reward / num_episodes"],"metadata":{"id":"G9bo7hfIf6b8","executionInfo":{"status":"ok","timestamp":1726987501928,"user_tz":420,"elapsed":9,"user":{"displayName":"Izaiah Benavides","userId":"18087988838344621844"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Extended Q_Learning"],"metadata":{"id":"UAbcrgTKh-KD"}},{"cell_type":"code","source":["import numpy as np\n","import gym\n","\n","# Create Frozen Lake environment\n","env = gym.make(\"FrozenLake-v1\")\n","\n","# Q-Learning algorithm function with optimizations\n","def q_learning_optimized(env, num_episodes=10000, max_steps=100, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, exploration_bonus=0.1):\n","    # Initialize Q-table with zeros\n","    Q = np.zeros((env.observation_space.n, env.action_space.n))\n","    exploration_count = np.zeros((env.observation_space.n, env.action_space.n))  # To track visits to each state-action pair\n","\n","    # Exploration function to favor less-explored actions\n","    def exploration_function(Q, state, action, exploration_count, exploration_bonus):\n","        return Q[state, action] + exploration_bonus / (exploration_count[state, action] + 1)  # Favor less-explored actions\n","\n","    # Function for Epsilon-Greedy policy\n","    def epsilon_greedy_action(state, Q, epsilon, exploration_count, exploration_bonus):\n","        if np.random.rand() < epsilon:\n","            return env.action_space.sample()  # Exploration\n","        else:\n","            # Choose action based on the exploration function\n","            exploration_q_values = [exploration_function(Q, state, action, exploration_count, exploration_bonus) for action in range(env.action_space.n)]\n","            return np.argmax(exploration_q_values)  # Exploitation using exploration function\n","\n","    # Q-Learning loop\n","    for episode in range(num_episodes):\n","        state = env.reset()\n","        done = False\n","\n","        for step in range(max_steps):\n","            # Select action using epsilon-greedy policy with exploration bonus\n","            action = epsilon_greedy_action(state, Q, epsilon, exploration_count, exploration_bonus)\n","\n","            # Perform action and observe the next state and reward\n","            next_state, reward, done, _ = env.step(action)\n","\n","            # Update the exploration count\n","            exploration_count[state, action] += 1\n","\n","            # Update Q-value\n","            best_next_action = np.argmax(Q[next_state])\n","            Q[state, action] = Q[state, action] + alpha * (reward + gamma * Q[next_state, best_next_action] - Q[state, action])\n","\n","            # Move to the next state\n","            state = next_state\n","\n","            if done:\n","                break\n","\n","        # Decay epsilon to reduce exploration over time\n","        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n","\n","    # Extract the optimal policy from Q-table\n","    optimal_policy = np.argmax(Q, axis=1)\n","    return Q, optimal_policy\n","\n","# Optimized Q-Learning\n","Q_table_optimized, optimal_policy_optimized = q_learning_optimized(env, exploration_bonus=0.1)  # Optimized version"],"metadata":{"id":"Jo4PpU_5iBPS","executionInfo":{"status":"ok","timestamp":1726987520533,"user_tz":420,"elapsed":18613,"user":{"displayName":"Izaiah Benavides","userId":"18087988838344621844"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Print Results"],"metadata":{"id":"bjJtZ_Tcqrcr"}},{"cell_type":"code","source":["# Evaluate the policy from Value Iteration\n","value_iteration_reward = evaluate_policy(env, optimal_policy_value_iteration)\n","\n","# Evaluate the policy from Q-Learning\n","q_learning_reward = evaluate_policy(env, optimal_policy_q_learning)\n","\n","# Evaluate policy with optimized Q-Learning\n","optimized_reward = evaluate_policy(env, optimal_policy_optimized)\n","\n","# Compute value function from Q-tables by taking the max Q-value for each state\n","values_q_learning = np.max(Q_table, axis=1)\n","values_optimized_q_learning = np.max(Q_table_optimized, axis=1)\n","\n","# Round the values to three decimal places\n","values_q_learning = np.round(values_q_learning, 3)\n","values_optimized_q_learning = np.round(values_optimized_q_learning, 3)\n","optimal_V = np.round(optimal_V, 3)\n","\n","# Print the average reward for all algorithms\n","print(f\"Average reward using Value Iteration: {value_iteration_reward}\")\n","print(f\"Average reward using Q-Learning: {q_learning_reward}\")\n","print(f\"Average reward with exploration bonus: {optimized_reward}\")\n","\n","# Print the policies for each algorithm\n","print(\"\\nPolicies and Values for each algorithm:\")\n","\n","# Value Iteration\n","print(\"Policy from Value Iteration:\")\n","print(optimal_policy_value_iteration.reshape((4, 4)))  # Reshape for the 4x4 Frozen Lake\n","print(\"\\nValues from Value Iteration:\")\n","print(optimal_V.reshape((4, 4)))\n","\n","# Q-Learning\n","print(\"\\nPolicy from Q-Learning:\")\n","print(optimal_policy_q_learning.reshape((4, 4)))  # Reshape for the 4x4 Frozen Lake\n","print(\"\\nValues from Q-Learning:\")\n","print(values_q_learning.reshape((4, 4)))\n","\n","# Optimized Q-Learning\n","print(\"\\nPolicy from Optimized Q-Learning:\")\n","print(optimal_policy_optimized.reshape((4, 4)))  # Reshape for the 4x4 Frozen Lake\n","print(\"\\nValues from Optimized Q-Learning:\")\n","print(values_optimized_q_learning.reshape((4, 4)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p6fuOOWNqYjK","executionInfo":{"status":"ok","timestamp":1726987522774,"user_tz":420,"elapsed":2246,"user":{"displayName":"Izaiah Benavides","userId":"18087988838344621844"}},"outputId":"6a3277f2-6d7c-4afc-b232-35cef4eb0b72"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Average reward using Value Iteration: 0.744\n","Average reward using Q-Learning: 0.702\n","Average reward with exploration bonus: 0.723\n","\n","Policies and Values for each algorithm:\n","Policy from Value Iteration:\n","[[0. 3. 0. 3.]\n"," [0. 0. 0. 0.]\n"," [3. 1. 0. 0.]\n"," [0. 2. 1. 0.]]\n","\n","Values from Value Iteration:\n","[[0.069 0.061 0.074 0.056]\n"," [0.092 0.    0.112 0.   ]\n"," [0.145 0.247 0.3   0.   ]\n"," [0.    0.38  0.639 0.   ]]\n","\n","Policy from Q-Learning:\n","[[0 1 0 3]\n"," [0 0 0 0]\n"," [3 1 0 0]\n"," [0 2 1 0]]\n","\n","Values from Q-Learning:\n","[[0.49  0.175 0.184 0.188]\n"," [0.509 0.    0.165 0.   ]\n"," [0.528 0.558 0.443 0.   ]\n"," [0.    0.683 0.855 0.   ]]\n","\n","Policy from Optimized Q-Learning:\n","[[0 3 0 3]\n"," [0 0 2 0]\n"," [3 1 0 0]\n"," [0 2 1 0]]\n","\n","Values from Optimized Q-Learning:\n","[[0.563 0.407 0.354 0.1  ]\n"," [0.591 0.    0.366 0.   ]\n"," [0.621 0.653 0.613 0.   ]\n"," [0.    0.735 0.874 0.   ]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"9e5xG2m8sDQj","executionInfo":{"status":"ok","timestamp":1726987522775,"user_tz":420,"elapsed":7,"user":{"displayName":"Izaiah Benavides","userId":"18087988838344621844"}}},"execution_count":5,"outputs":[]}]}